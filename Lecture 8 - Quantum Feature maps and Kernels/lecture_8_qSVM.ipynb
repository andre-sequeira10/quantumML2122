{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantum Data Science 2021/2022\n",
    "# Lecture 8 - Quantum Feature maps, Kernels and Support Vector Machines\n",
    "\n",
    "<!-- no toc -->\n",
    "### Contents \n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Support Vector Machines](#svm)\n",
    "3. [Caveats](#caveats)\n",
    "4. [Non-linear separation via feature maps](#non-linear)\n",
    "5. [Support Vector Machines with Kernels](#kernels)\n",
    "6. [Data encoding as a Quantum Feature map](#qfeaturemap)\n",
    "7. [Quantum Kernel estimation](#qke)\n",
    "8. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Introduction <a id=\"intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a label dataset dataset with $M$ points of 2 feature only , i.e $x_i \\in \\mathbb{R}^2$\n",
    "$$ \\mathcal{X} = \\{(x_0^0, x_1^0), (x_0^1, x_1^1), \\dots, (x_0^{M-1}, x_1^{M-1}) \\}$$\n",
    "\n",
    "with two classes: \n",
    "$$ \\mathcal{Y} \\in \\{-1,1\\}$$ \n",
    "\n",
    "Consider a 2D plot of the data with multiple decision boundaries:\n",
    "\n",
    "<p align=\"center\">\n",
    " <img width=\"800\" height=\"400\" src=\"images/decision_boundaries.jpeg\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">Question: </span> What hypothesis class should we choose? \n",
    "\n",
    "(3) looks very expressive, therefore we should choose it, right?\n",
    "No, notice that every hypothesis correctly classify the entire dataset. Therefore, we should choose the minimum complexity hypothesis that works ! ---> **Occam's Razor**. This is again because of the problem of overfitting and generalization! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, a Linear classifier should be chosen! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{H} = \\{x \\mapsto sign(w^T x + b) : x \\in \\mathbb{R}^2, w \\in \\mathbb{R}^2, b \\in \\mathbb{R} \\}$$ \n",
    "\n",
    "The sign operator returns the sign of the operation inside the brackets. For simplicity we're considering $\\mathbb{R}^2$, but this could be very well generalized to larger dimensions -> $\\mathbb{R}^N$\n",
    "\n",
    "Remember that in this case, the general hyperplane equation is:\n",
    "\n",
    "$$ w.x + b = 0$$ \n",
    "\n",
    "<p align=\"center\">\n",
    " <img width=\"500\" height=\"500\" src=\"images/r3.jpeg\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">Question: </span> But, there are infinite hyperplanes that correctly separate the two classes ! How do we choose the best ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Support Vector Machines <a id=\"svm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should choose the model that minimizes the margin $\\gamma$ i.e. the model with smallest distance between the hyperplane and a single point in the dataset.\n",
    "\n",
    "<p align=\"center\">\n",
    " <img width=\"600\" height=\"500\" src=\"images/support_vectors.jpg\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating the margin:\n",
    "\n",
    "Consider the following picture:\n",
    "\n",
    "<p align=\"center\">\n",
    " <img width=\"600\" height=\"500\" src=\"images/gamma.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\frac{w}{||w||}$ be the unit length vector. We know that point B can be calculated as: \n",
    "\n",
    "$$A- \\gamma \\frac{w}{||w||}$$\n",
    "for a point in the positive side of the hyperplane. Thus, putting back in the hyperplane and solving for $\\gamma$ , with an arbitrary point $x_i$ leads to: \n",
    "\n",
    "$$\\gamma_i = \\frac{w^T x_i + b}{||w||}$$\n",
    "\n",
    "In general, for any point, the margin can be calculated inserting the corresponding label:\n",
    "\n",
    "$$\\gamma_i = y_i(\\frac{w^T x_i + b}{||w||})$$\n",
    "\n",
    "Notice that $\\gamma_i$ is always $>0$ once the prediction is correct, i.e fell on the correct side of the hyperplane . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always restrict ourselves to margin being equal to one, simplifying the problem! \n",
    "\n",
    "<p align=\"center\">\n",
    " <img width=\"800\" height=\"400\" src=\"images/margin=1.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leading to the well known **primal formulation** for the optimization problem in support vector machines.\n",
    "\n",
    "$$ max_{w,b} \\frac{1}{||w||} = min_{w,b} \\frac{1}{2} ||w||^2 $$\n",
    "\n",
    "subject to constraints :\n",
    "\n",
    "$$ y_i(w^T x_i + b) \\geq 1 $$\n",
    "\n",
    "This can be solved with off the shelf software! E.g Sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Caveats <a id=\"caveats\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Differentiable function and $\\nabla_w \\frac{1}{2} ||w||^2 = w$ and $\\nabla_w^2 \\frac{1}{2} ||w||^2 = 1$\n",
    "* Eigenvalues are $>0$ thus the function is **stricly convex**. This is awesome because it tell us that we have a unique solution - Any local minimum is the global minimum. This is not true is general neural networks algorithms. Here there are well established theoretical guarantees. For instance the the number of datapoints necessary to generalize and guarantee a minimum prediction error - Sample complexity. See [1]\n",
    "  \n",
    "<p align=\"center\">\n",
    " <img width=\"800\" height=\"400\" src=\"images/convex.jpeg\">\n",
    "</p>\n",
    "\n",
    "* However, recall that $w \\in \\mathbb{R}^N$. Thus when we have a million features , the optimization becomes prohibitive. Thus , the **dual formulation** is often used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}(w,\\alpha,b) = \\frac{1}{2} ||w||^2 - \\sum_{i=1}^{M} \\alpha_i (y_i(w^T x_i +b) - 1)$$\n",
    "\n",
    "considering: \n",
    "\n",
    "$$\\nabla_{w} L=w-\\sum_{i=1}^{M} \\alpha_{i} y_{i} x_{i}=0 \\quad \\Rightarrow \\quad w=\\sum_{i=1}^{M} \\alpha_{i} y_{i} x_{i}$$\n",
    "\n",
    "$$\\nabla_{b} v=-\\sum_{i=1}^{M} \\alpha_{i} y_{i}=0 \\quad \\Rightarrow \\quad \\sum_{i=1}^{M} \\alpha_{i} y_{i}=0$$\n",
    "\n",
    "$$\\forall_i, \\alpha_{i}\\left[y_{i}\\left(w^{\\top}, x_{i}+b\\right)-1\\right]=0 \\Rightarrow \\alpha_{i} \\approx 0 \\vee y_{i}\\left(w_{i} x_{i}+b\\right)=1$$\n",
    "\n",
    "Implying that now we just need to optimize as many $\\alpha$'s as the number of training points in our dataset. Moreover, $\\alpha = 1$ only for support vectors which are much fewer than the total number of points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing $w$ and $b$, we get the most used expression for support vector machines: \n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\max _{\\alpha} \\mathcal{L}(\\alpha) \\\\ \n",
    "\\\\\n",
    "\\max _{\\alpha} \\sum \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{M} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_i^T  x_j\\right) \\\\\n",
    "\\\\\n",
    "\\text { s.t } \\alpha_{i} \\geqslant 0 \\wedge \\sum_{i=1}^{M} \\alpha_{i} y_{i}=0\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learning algorithms based on SVM's are efficiently learnable !!! In the language of **PAC Learning**\n",
    "  \n",
    "  --- \n",
    "  \n",
    "  **PAC LEARNING:** \n",
    "  \n",
    "  *Probably approximately correct* - The number of samples that we need to have an $\\epsilon$-approximation of the empirical error. \n",
    "\n",
    "  $$ \\mathbb{P} \\left[ R \\leq \\epsilon\\right] \\geq 1 - \\delta$$\n",
    "\n",
    "  An algorithm is PAC Learnable if the number of samples grows only polylogarithmically with error, confidence interval, size of hypothesis ... \n",
    "\n",
    "  ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $x_i^T x_i$ is the **inner product** between datapoints $i$ and $j$. \n",
    "\n",
    "<span style=\"color: red;\">Question: </span> What if the data is not linearly separable? \n",
    "\n",
    "Enters Kernels ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Non linearly separable data <a id=\"nonlinear\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature maps: \n",
    "The hope is that we can apply a non-linear transformation to the data, mapping our points to a larger dimension space $\\mathcal{H}$ -> Feature space. \n",
    "\n",
    "$$ \\phi: \\mathcal{X} \\mapsto \\mathcal{H} $$\n",
    "\n",
    "<p align=\"center\">\n",
    " <img width=\"900\" height=\"400\" src=\"images/concentric circles.png\">\n",
    "</p>\n",
    "\n",
    "<span style=\"color: red;\">Question: </span> But the feature vectors will not become too big? Well, there are even infinite dimensional feature spaces. For instance, via an exponential feature map! \n",
    "\n",
    "We don't need to work with feature vectors provided we can estimate inner products! Provided we can compare vectors in large feature spaces! \n",
    "\n",
    "Enters kernels and the kernel trick !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels : \n",
    "\n",
    "The Kernel $K$ is a similarity measure between two points $x_i$ and $x_j$ in feature space: \n",
    "\n",
    "$$ K: \\Omega \\times \\Omega \\mapsto \\mathbb{R} $$\n",
    "$$ K: (x,y) \\longmapsto k(x,y)=\\langle\\phi(x) , \\phi(y)\\rangle $$\n",
    "\n",
    "The Kernel trick allow us to compare points in high dimensional feature spaces by using the original input vectors! \n",
    "\n",
    "**Mercer's condition**: $k(x,y) \\geq 0$  and the kernel matrix $\\mathcal{K}$:\n",
    "\n",
    "$$ \\mathcal{K} = \\left[ k(x,y) \\right]_{xy}$$ \n",
    "\n",
    "is positive semidefinite. Convexity !\n",
    "\n",
    "Take a look at well-known Kernels:\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    " <img width=\"/00\" height=\"300\" src=\"images/kernels.png\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Support Vector Machines with Kernels <a id=\"kernels\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the dual formulation but now replacing the inner product with the kernel:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\max _{\\alpha} \\mathcal{L}(\\alpha) \\\\ \n",
    "\\\\\n",
    "\\max _{\\alpha} \\sum \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{M} \\alpha_{i} \\alpha_{j} y_{i} y_{j} \\langle\\phi(x) , \\phi(y)\\rangle \\\\\n",
    "\\\\\n",
    "\\text { s.t } \\alpha_{i} \\geqslant 0 \\wedge \\sum_{i=1}^{M} \\alpha_{i} y_{i}=0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "### <span style=\"color: red;\">Question: </span>\n",
    " But wait a minute ? How do we choose these kernels ? Do we need to make the data to higher dimension feature spaces, or lower dimension? \n",
    "\n",
    "--- \n",
    "Answer: Lot's of engineering needed based on feature knowledge! \n",
    "\n",
    "---> This is the reason for Neural Networks being used nowadays , because they find non-linear decision boundaries on their own, without needing a human to design a kernel\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Quantum Feature maps and kernels <a id=\"qfeaturemap\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Represent the kernel function with a quantum computer! Why?\n",
    "Because quantum computers already work in high dimensional feature spaces. \n",
    "\n",
    "We can simply use the data encoding as a feature map !\n",
    "\n",
    "Then we just need to find a way of calculating inner products between point in feature space, which we already know how to do ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Feature map:\n",
    "\n",
    "$$ \\phi: x \\mapsto |\\phi(x) \\rangle $$\n",
    "\n",
    "### Quantum kernel: \n",
    "\n",
    "For two points $x$ and $y$ , the kernel can be obtained from the well-known fidelity: \n",
    "\n",
    "$$k(x,y) = |\\langle \\phi(x) | \\phi(y) \\rangle|^2 $$\n",
    "\n",
    "This is indeed a kernel. Notice that this is also positive semidefinite ( $k(x,y) \\geq 0 $)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    " <img width=\"/00\" height=\"300\" src=\"images/kernel_types.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">Question: </span> How can we calculate the fidelity? \n",
    "\n",
    "Can we do better? \n",
    "\n",
    "<span style=\"color: red;\">Exercise: </span> Derive a quantum circuit to estimate the fidelity. Compare with the swap test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    " <img width=\"600\" height=\"300\" src=\"images/inverse_test.png\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point is: Why do we need a quantum computer to estimate the kernels described above? Yes, ESTIMATE! Why do we estimate? \n",
    "\n",
    "Well, the answer is ... we don't!! Classical computers can efficiently compute said kernels. Therefore, the goal here is to exploit quantum processes that are not efficiently simulatable by a classical computer and create powerful feature maps! \n",
    "\n",
    "One such example is the IQP encoding: \n",
    "\n",
    "<p align=\"center\">\n",
    " <img width=\"800\" height=\"250\" src=\"images/IQP.png\">\n",
    "</p>\n",
    "\n",
    "which is conjectured to be hard to simulate classically, as we increase the number of layers. \n",
    "\n",
    "### GOAL : Design feature maps that are intractable to simulate classically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Quantum Kernel estimation <a id=\"qke\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noticed above, if we measure the probability of basis state $|0\\rangle$ , we measure the kernel ! \n",
    "\n",
    "$$ \\mathbb{P}(|0\\rangle) = |\\langle \\phi(x) | \\phi(y) \\rangle|^2 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need many shots to estimate the probability, depending on the type of kernel, the number of shots may remove any quantum advantage. However, it seems that we can estimate only with additive sampling error, which is good! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - References <a id=\"references\"></a>\n",
    "\n",
    "* [1] - Mohri, Mehryar et al. “Foundations of Machine Learning.” Adaptive computation and machine learning (2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ciao for now :) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62314c999a2395b70c6666201aadb026cfae52075e045b8f587202e06e575832"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('quantum_DS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
